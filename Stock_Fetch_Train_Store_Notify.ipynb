{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6lBLujK6RA5+Q1qUnadDU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/macosta/ServiceNow-JavaScript-Training/blob/master/Stock_Fetch_Train_Store_Notify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Necessary Packages**\n",
        "\n"
      ],
      "metadata": {
        "id": "q2yAaivV4-G3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance scikit-learn\n",
        "!pip install --upgrade yfinance\n",
        "!pip install retrying\n",
        "!pip install supabase"
      ],
      "metadata": {
        "id": "nWYG1aggAt5T",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d8a26c9-9519-4e0c-9700-acdf277ac7e6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.10/dist-packages (0.2.40)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.0.3)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.25.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.31.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.9.4)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.2.2)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2023.4)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.4.4)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.10/dist-packages (from yfinance) (3.17.6)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.12.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2024.7.4)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.10/dist-packages (0.2.40)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.0.3)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.25.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.31.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.9.4)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.2.2)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2023.4)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.4.4)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.10/dist-packages (from yfinance) (3.17.6)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.12.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2024.7.4)\n",
            "Collecting retrying\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from retrying) (1.16.0)\n",
            "Installing collected packages: retrying\n",
            "Successfully installed retrying-1.3.4\n",
            "Collecting supabase\n",
            "  Downloading supabase-2.5.1-py3-none-any.whl (16 kB)\n",
            "Collecting gotrue<3.0,>=1.3 (from supabase)\n",
            "  Downloading gotrue-2.5.4-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m811.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx<0.28,>=0.24 (from supabase)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting postgrest<0.17.0,>=0.14 (from supabase)\n",
            "  Downloading postgrest-0.16.8-py3-none-any.whl (21 kB)\n",
            "Collecting realtime<2.0.0,>=1.0.0 (from supabase)\n",
            "  Downloading realtime-1.0.6-py3-none-any.whl (9.0 kB)\n",
            "Collecting storage3<0.8.0,>=0.5.3 (from supabase)\n",
            "  Downloading storage3-0.7.6-py3-none-any.whl (16 kB)\n",
            "Collecting supafunc<0.5.0,>=0.3.1 (from supabase)\n",
            "  Downloading supafunc-0.4.6-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.10 in /usr/local/lib/python3.10/dist-packages (from gotrue<3.0,>=1.3->supabase) (2.8.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28,>=0.24->supabase) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.28,>=0.24->supabase) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<0.28,>=0.24->supabase)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<0.28,>=0.24->supabase) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28,>=0.24->supabase) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<0.28,>=0.24->supabase)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deprecation<3.0.0,>=2.1.0 (from postgrest<0.17.0,>=0.14->supabase)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: strenum<0.5.0,>=0.4.9 in /usr/local/lib/python3.10/dist-packages (from postgrest<0.17.0,>=0.14->supabase) (0.4.15)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from realtime<2.0.0,>=1.0.0->supabase) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from realtime<2.0.0,>=1.0.0->supabase) (4.12.2)\n",
            "Collecting websockets<13,>=11 (from realtime<2.0.0,>=1.0.0->supabase)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from deprecation<3.0.0,>=2.1.0->postgrest<0.17.0,>=0.14->supabase) (24.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.10->gotrue<3.0,>=1.3->supabase) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.10->gotrue<3.0,>=1.3->supabase) (2.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.8.1->realtime<2.0.0,>=1.0.0->supabase) (1.16.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28,>=0.24->supabase) (1.2.1)\n",
            "Installing collected packages: websockets, h11, deprecation, realtime, httpcore, httpx, supafunc, storage3, postgrest, gotrue, supabase\n",
            "Successfully installed deprecation-2.1.0 gotrue-2.5.4 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 postgrest-0.16.8 realtime-1.0.6 storage3-0.7.6 supabase-2.5.1 supafunc-0.4.6 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "YEYpjYLcAxOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import logging\n",
        "import time\n",
        "from supabase import create_client, Client"
      ],
      "metadata": {
        "id": "6KEHh9IaA0fN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Environment Variables**"
      ],
      "metadata": {
        "id": "Jldns2EpCTiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"SUPABASE_URL\"] = \"https://zphgtxuwecitedpxrahe.supabase.co\"\n",
        "os.environ[\"SUPABASE_KEY\"] = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InpwaGd0eHV3ZWNpdGVkcHhyYWhlIiwicm9sZSI6ImFub24iLCJpYXQiOjE3MTkzNTg3NjMsImV4cCI6MjAzNDkzNDc2M30.oAtpm0gYUEihPSkXtvpf0v_mu6H3lYqUYfcQVt-tI2w\"\n"
      ],
      "metadata": {
        "id": "l2eGvXf5CWLv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Initialize Supabase Connection**"
      ],
      "metadata": {
        "id": "tcO3XPcIA3HB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from supabase import create_client, Client\n",
        "\n",
        "def initialize_supabase():\n",
        "    url = os.getenv(\"SUPABASE_URL\")\n",
        "    key = os.getenv(\"SUPABASE_KEY\")\n",
        "    client = create_client(url, key)\n",
        "    return client\n",
        "\n",
        "# Initialize Supabase\n",
        "client = initialize_supabase()\n"
      ],
      "metadata": {
        "id": "xoTU7NdHA7fV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fetch Company Information**"
      ],
      "metadata": {
        "id": "BLDzr8jkBLdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from supabase import create_client, Client\n",
        "\n",
        "# Initialize Supabase\n",
        "def initialize_supabase():\n",
        "    url = os.getenv(\"SUPABASE_URL\")\n",
        "    key = os.getenv(\"SUPABASE_KEY\")\n",
        "    client = create_client(url, key)\n",
        "    return client\n",
        "\n",
        "client = initialize_supabase()\n",
        "\n",
        "# Fetch company information from Supabase\n",
        "def fetch_company_information(client):\n",
        "    try:\n",
        "        response = client.table(\"Stock Company Information\").select(\"*\").execute()\n",
        "        company_data = response.data\n",
        "        if company_data:\n",
        "            company_df = pd.DataFrame(company_data)\n",
        "            symbols = company_df['symbol'].tolist()\n",
        "            company_names = company_df['company_name'].tolist()\n",
        "            return company_df, symbols, company_names\n",
        "        else:\n",
        "            raise ValueError(\"No data found in the 'Stock Company Information' table.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching company information: {e}\")\n",
        "        return pd.DataFrame(), [], []\n",
        "\n",
        "# Get company symbols and names\n",
        "company_df, symbols, company_names = fetch_company_information(client)\n"
      ],
      "metadata": {
        "id": "ePsZZfJmBQqc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fetch Stock Data for Batch of Symbols**"
      ],
      "metadata": {
        "id": "9WclprMJBVYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf  # Import the yfinance library for fetching stock data\n",
        "import pandas as pd  # Import pandas for data manipulation\n",
        "import numpy as np  # Import numpy for numerical operations\n",
        "import logging  # Import logging for logging messages\n",
        "import time  # Import time for sleep function\n",
        "import os  # Import os for environment variable handling\n",
        "import json  # Import json for JSON handling\n",
        "from supabase import create_client, Client  # Import Supabase client for database operations\n",
        "\n",
        "# Set up logging configuration\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[\n",
        "    logging.FileHandler(\"model_training.log\"),  # Log to a file\n",
        "    logging.StreamHandler()  # Log to the console\n",
        "])\n",
        "\n",
        "# Set environment variables for Supabase connection\n",
        "os.environ[\"SUPABASE_URL\"] = \"https://zphgtxuwecitedpxrahe.supabase.co\"\n",
        "os.environ[\"SUPABASE_KEY\"] = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InpwaGd0eHV3ZWNpdGVkcHhyYWhlIiwicm9sZSI6ImFub24iLCJpYXQiOjE3MTkzNTg3NjMsImV4cCI6MjAzNDkzNDc2M30.oAtpm0gYUEihPSkXtvpf0v_mu6H3lYqUYfcQVt-tI2w\"  # Replace with your actual Supabase anon key\n",
        "\n",
        "# Function to initialize Supabase connection\n",
        "def initialize_supabase():\n",
        "    url = os.getenv(\"SUPABASE_URL\")  # Get Supabase URL from environment variable\n",
        "    key = os.getenv(\"SUPABASE_KEY\")  # Get Supabase key from environment variable\n",
        "    client = create_client(url, key)  # Create Supabase client\n",
        "    return client\n",
        "\n",
        "# Initialize Supabase\n",
        "client = initialize_supabase()\n",
        "\n",
        "# Constants for file paths, batch size, and date range\n",
        "PROGRESS_FILE = 'progress.csv'\n",
        "BATCH_SIZE = 10\n",
        "START_DATE = \"2018-01-01\"\n",
        "END_DATE = None  # No end date, fetch data up to the current date\n",
        "\n",
        "# Function to add features to the stock data\n",
        "def add_features(data, symbol, info):\n",
        "    data.columns = data.columns.str.lower()  # Convert column names to lowercase\n",
        "    data['symbol'] = symbol  # Add symbol column\n",
        "    data['company_name'] = info.get('longName', 'N/A')  # Add company name column\n",
        "    data['active'] = True  # Mark as active by default\n",
        "    data['dividends'] = info.get('trailingAnnualDividendYield', 0)  # Add dividends column\n",
        "    data['stock_splits'] = info.get('lastSplitFactor', 0)  # Add stock splits column\n",
        "    data['dividend_yield'] = info.get('dividendYield', 0)  # Add dividend yield column\n",
        "    data['payout_ratio'] = info.get('payoutRatio', 0)  # Add payout ratio column\n",
        "    data['free_cash_flow'] = info.get('freeCashflow', 0)  # Add free cash flow column\n",
        "    data['return_on_equity'] = info.get('returnOnEquity', 0)  # Add return on equity column\n",
        "    data['earnings_per_share_eps'] = info.get('trailingEps', 0)  # Add earnings per share column\n",
        "    data['price_to_earnings_ratio'] = info.get('trailingPE', 0)  # Add price-to-earnings ratio column\n",
        "    data['market_capitalization'] = info.get('marketCap', 0)  # Add market capitalization column\n",
        "    data['revenue'] = info.get('totalRevenue', 0)  # Add revenue column\n",
        "    data['net_income'] = info.get('netIncomeToCommon', 0)  # Add net income column\n",
        "    data['total_assets'] = info.get('totalAssets', 0)  # Add total assets column\n",
        "    data['total_liabilities'] = info.get('totalLiab', 0)  # Add total liabilities column\n",
        "    data['total_equity'] = data['total_assets'] - data['total_liabilities']  # Calculate total equity\n",
        "    data['operating_cash_flow'] = info.get('operatingCashflow', 0)  # Add operating cash flow column\n",
        "    data['investing_cash_flow'] = info.get('investingCashflow', 0)  # Add investing cash flow column\n",
        "    data['financing_cash_flow'] = info.get('financingCashflow', 0)  # Add financing cash flow column\n",
        "    data['sma_50'] = data['close'].rolling(window=50).mean()  # Calculate 50-day SMA\n",
        "    data['volatility'] = data['close'].rolling(window=50).std()  # Calculate 50-day volatility\n",
        "    data['ema'] = data['close'].ewm(span=20, adjust=False).mean()  # Calculate 20-day EMA\n",
        "    data['model_trained'] = False  # Set model trained status to False\n",
        "    data['created_at'] = pd.Timestamp.utcnow()  # Add created at timestamp as Timestamp object\n",
        "    return data\n",
        "\n",
        "# Function to check stock status\n",
        "def check_stock_status(symbol):\n",
        "    try:\n",
        "        stock = yf.Ticker(symbol)  # Create a Ticker object\n",
        "        info = stock.info  # Fetch stock info\n",
        "        if 'quoteType' in info and info['quoteType'] in ['EQUITY', 'ETF']:  # Check if the stock is active\n",
        "            logging.info(f\"{symbol} is active with quoteType: {info['quoteType']}\")\n",
        "            return True, info\n",
        "        else:\n",
        "            logging.warning(f\"{symbol} might be delisted or have no market data available.\")\n",
        "            return False, None\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error checking status for {symbol}: {e}\")\n",
        "        return False, None\n",
        "\n",
        "# Function to fetch stock data\n",
        "def fetch_stock_data(symbols, start_date, end_date):\n",
        "    data = {}  # Initialize dictionary to store data for each symbol\n",
        "    for symbol in symbols:\n",
        "        valid, info = check_stock_status(symbol)  # Check if the stock is valid\n",
        "        if valid:\n",
        "            try:\n",
        "                stock = yf.Ticker(symbol)  # Create a Ticker object\n",
        "                stock_data = stock.history(start=start_date, end=end_date)  # Fetch historical data\n",
        "                if not stock_data.empty:\n",
        "                    stock_data = add_features(stock_data, symbol, info)  # Add features to the data\n",
        "                    stock_data.index = pd.to_datetime(stock_data.index)  # Ensure the index is datetime\n",
        "\n",
        "                    # Check if the index is properly converted to datetime\n",
        "                    if not pd.api.types.is_datetime64_any_dtype(stock_data.index):\n",
        "                        logging.error(f\"Date index for symbol {symbol} could not be converted to datetime.\")\n",
        "                        continue\n",
        "\n",
        "                    if stock_data.index.tzinfo is None or stock_data.index.tzinfo.utcoffset(stock_data.index) is None:\n",
        "                        stock_data.index = stock_data.index.tz_localize('UTC')  # Localize to UTC if not already\n",
        "                    else:\n",
        "                        stock_data.index = stock_data.index.tz_convert('UTC')  # Convert to UTC if already localized\n",
        "\n",
        "                    stock_data.rename_axis('date', inplace=True)  # Rename index to date\n",
        "\n",
        "                    # Validate the date column\n",
        "                    if (stock_data.index == 0).any():\n",
        "                        logging.error(f\"Invalid date found in stock data for symbol {symbol}\")\n",
        "                        logging.error(f\"Problematic dates: {stock_data.index[stock_data.index == 0]}\")\n",
        "                        stock_data.index = pd.date_range(start=start_date, periods=len(stock_data), freq='D')  # Fix invalid dates\n",
        "\n",
        "                    data[symbol] = stock_data  # Store the data\n",
        "                    logging.info(f\"Fetched and added features for {symbol}\")\n",
        "                else:\n",
        "                    logging.warning(f\"No data fetched for {symbol}\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error fetching data for {symbol}: {e}\")\n",
        "        else:\n",
        "            # Create an empty DataFrame with predefined columns if the stock symbol is invalid\n",
        "            stock_data = pd.DataFrame(columns=[\n",
        "                \"date\", \"symbol\", \"company_name\", \"open\", \"high\", \"low\", \"close\", \"adj_close\", \"volume\",\n",
        "                \"dividends\", \"stock_splits\", \"dividend_yield\", \"payout_ratio\", \"free_cash_flow\", \"return_on_equity\",\n",
        "                \"earnings_per_share_eps\", \"price_to_earnings_ratio\", \"market_capitalization\", \"revenue\", \"net_income\",\n",
        "                \"total_assets\", \"total_liabilities\", \"total_equity\", \"operating_cash_flow\", \"investing_cash_flow\",\n",
        "                \"financing_cash_flow\", \"sma_50\", \"volatility\", \"ema\", \"active\", \"model_trained\", \"created_at\"\n",
        "            ])\n",
        "            stock_data.loc[0] = [None] * (len(stock_data.columns) - 1) + [False]  # Mark as inactive\n",
        "            data[symbol] = stock_data\n",
        "            logging.info(f\"Skipping {symbol} due to validation issues.\")\n",
        "        time.sleep(1)  # Sleep to avoid hitting rate limits\n",
        "    return data\n",
        "\n",
        "# Function to load progress from a file\n",
        "def load_progress(progress_file):\n",
        "    if os.path.exists(progress_file):\n",
        "        return pd.read_csv(progress_file)['symbol'].tolist()  # Load processed symbols\n",
        "    return []\n",
        "\n",
        "# Function to save progress to a file\n",
        "def save_progress(progress_file, processed_symbols):\n",
        "    pd.DataFrame({'symbol': processed_symbols}).to_csv(progress_file, index=False)  # Save processed symbols\n",
        "\n",
        "# Function to ensure correct format for timestamp columns\n",
        "def format_timestamps(data):\n",
        "    for column in data.columns:\n",
        "        if pd.api.types.is_datetime64_any_dtype(data[column]):\n",
        "            data[column] = data[column].apply(lambda x: x.isoformat() if pd.notnull(x) else None)\n",
        "    return data\n",
        "\n",
        "# Function to serialize complex fields to JSON\n",
        "def serialize_complex_fields(data):\n",
        "    for column in data.columns:\n",
        "        if isinstance(data[column].iloc[0], (dict, list)):\n",
        "            data[column] = data[column].apply(lambda x: json.dumps(x) if pd.notnull(x) else None)\n",
        "    return data\n",
        "\n",
        "# Function to validate data and store it in Supabase\n",
        "def validate_and_store_data(client, data, company_df):\n",
        "    # Define the headers\n",
        "    headers = [\n",
        "        \"date\", \"symbol\", \"company_name\", \"open\", \"high\", \"low\", \"close\", \"adj_close\", \"volume\",\n",
        "        \"dividends\", \"stock_splits\", \"dividend_yield\", \"payout_ratio\", \"free_cash_flow\", \"return_on_equity\",\n",
        "        \"earnings_per_share_eps\", \"price_to_earnings_ratio\", \"market_capitalization\", \"revenue\", \"net_income\",\n",
        "        \"total_assets\", \"total_liabilities\", \"total_equity\", \"operating_cash_flow\", \"investing_cash_flow\",\n",
        "        \"financing_cash_flow\", \"sma_50\", \"volatility\", \"ema\", \"active\", \"model_trained\", \"created_at\"\n",
        "    ]\n",
        "\n",
        "    for symbol, stock_data in data.items():\n",
        "        if not stock_data.empty:\n",
        "            stock_data.reset_index(drop=True, inplace=True)  # Reset index to use default integer index\n",
        "            stock_data['symbol'] = symbol  # Add symbol column\n",
        "            stock_data['company_name'] = company_df[company_df['symbol'] == symbol]['company_name'].iloc[0]  # Add company name\n",
        "\n",
        "            # Placeholder values for missing columns\n",
        "            for col in headers:\n",
        "                if col not in stock_data.columns:\n",
        "                    stock_data[col] = None\n",
        "\n",
        "            # Reindex columns to match the headers\n",
        "            stock_data = stock_data.reindex(columns=headers, fill_value=None)\n",
        "\n",
        "            # Replace out-of-range float values and NaNs\n",
        "            stock_data.replace([float('inf'), float('-inf')], None, inplace=True)\n",
        "            stock_data = stock_data.where(pd.notnull(stock_data), None)\n",
        "\n",
        "            # Ensure correct format for timestamp columns\n",
        "            stock_data = format_timestamps(stock_data)\n",
        "\n",
        "            # Serialize complex fields to JSON\n",
        "            stock_data = serialize_complex_fields(stock_data)\n",
        "\n",
        "            # Convert boolean columns to integers (Supabase does not support boolean values)\n",
        "            boolean_columns = stock_data.select_dtypes(include=[bool]).columns.tolist()\n",
        "            for col in boolean_columns:\n",
        "                stock_data[col] = stock_data[col].astype(int)\n",
        "\n",
        "            # Handle NaN values in critical columns\n",
        "            critical_columns = ['adj_close', 'sma_50', 'volatility']\n",
        "            for col in critical_columns:\n",
        "                if col in stock_data.columns:\n",
        "                    stock_data[col].fillna(0, inplace=True)  # Or use another appropriate value\n",
        "                    stock_data[col] = stock_data[col].apply(lambda x: 0 if pd.isnull(x) else x)  # Ensure no NaNs\n",
        "\n",
        "            # Handle NaN values in the date column separately\n",
        "            if 'date' in stock_data.columns:\n",
        "                stock_data['date'] = pd.to_datetime(stock_data['date'], errors='coerce')  # Convert invalid dates to NaT\n",
        "                stock_data['date'].fillna(pd.Timestamp(\"1970-01-01\", tz='UTC'), inplace=True)  # Replace NaT with a default date\n",
        "\n",
        "            # Log problematic rows\n",
        "            for index, row in stock_data.iterrows():\n",
        "                if pd.isnull(row['date']) or isinstance(row['date'], int) or not isinstance(row['date'], pd.Timestamp):\n",
        "                    logging.error(f\"Invalid date found in row {index} for symbol {symbol}. Row: {row}\")\n",
        "                    logging.error(f\"Problematic date value: {row['date']} (type: {type(row['date'])})\")\n",
        "                    logging.error(f\"Full problematic row: {row.to_dict()}\")\n",
        "                    # Optionally, you can drop the row or set a default value\n",
        "                    stock_data.at[index, 'date'] = pd.Timestamp(\"1970-01-01\", tz='UTC')  # Example default date\n",
        "                    # Uncomment the next line to drop the problematic row instead of fixing it\n",
        "                    # stock_data.drop(index, inplace=True)\n",
        "\n",
        "            # Log request data before sending to Supabase\n",
        "            logging.info(f\"Request Data for {symbol}: {stock_data.to_dict(orient='records')}\")\n",
        "\n",
        "            # Convert all dates to strings to ensure JSON serializability\n",
        "            stock_data['date'] = stock_data['date'].apply(lambda x: x.isoformat() if isinstance(x, pd.Timestamp) else x)\n",
        "            if 'created_at' in stock_data.columns:\n",
        "                stock_data['created_at'] = stock_data['created_at'].apply(lambda x: x.isoformat() if isinstance(x, pd.Timestamp) else x)\n",
        "\n",
        "            # Insert data into Supabase\n",
        "            try:\n",
        "                response = client.table('Historical Stock Dividend Data').insert(stock_data.to_dict(orient='records')).execute()\n",
        "                logging.info(f\"Data for {symbol} stored successfully in Supabase. Response: {response}\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error storing data for {symbol} in Supabase: {e}\")\n",
        "                if hasattr(e, 'response') and e.response:\n",
        "                    logging.error(f\"Response: {e.response.json()}\")\n",
        "                else:\n",
        "                    logging.error(\"No response data\")\n",
        "\n",
        "                # Log details of the problematic column\n",
        "                for col in stock_data.columns:\n",
        "                    if stock_data[col].isnull().any():\n",
        "                        logging.error(f\"Column causing error: {col}\")\n",
        "        else:\n",
        "            logging.warning(f\"No data fetched for {symbol}\")\n",
        "\n",
        "# Fetch company data from Supabase\n",
        "response = client.table(\"Stock Company Information\").select(\"*\").execute()\n",
        "company_df = pd.DataFrame(response.data)  # Convert response data to DataFrame\n",
        "symbols = company_df['symbol'].tolist()  # Get list of symbols\n",
        "\n",
        "# Example data processing and storing in Supabase\n",
        "processed_symbols = load_progress(PROGRESS_FILE)  # Load progress from file\n",
        "symbols_to_process = company_df[~company_df['symbol'].isin(processed_symbols)]  # Get symbols to process\n",
        "\n",
        "for i in range(0, len(symbols_to_process), BATCH_SIZE):\n",
        "    batch = symbols_to_process.iloc[i:i + BATCH_SIZE]  # Process in batches\n",
        "    symbols = batch['symbol'].tolist()  # Get list of symbols in batch\n",
        "    stock_data_batch = fetch_stock_data(symbols, START_DATE, END_DATE)  # Fetch stock data\n",
        "\n",
        "    for _, company in batch.iterrows():\n",
        "        symbol = company['symbol']\n",
        "        if symbol in stock_data_batch:\n",
        "            stock_data = stock_data_batch[symbol]\n",
        "            # Validate data and store in Supabase\n",
        "            validate_and_store_data(client, {symbol: stock_data}, company_df)\n",
        "            processed_symbols.append(symbol)\n",
        "            save_progress(PROGRESS_FILE, processed_symbols)  # Save progress\n",
        "        else:\n",
        "            logging.warning(f\"No data fetched for {symbol}\")\n"
      ],
      "metadata": {
        "id": "h-2HALXaCj7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validate Data and Store it in Supabase**"
      ],
      "metadata": {
        "id": "_4P9wUR2lkkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf  # Import the yfinance library for fetching stock data\n",
        "import pandas as pd  # Import pandas for data manipulation\n",
        "import numpy as np  # Import numpy for numerical operations\n",
        "import logging  # Import logging for logging messages\n",
        "import os  # Import os for environment variable handling\n",
        "import json  # Import json for JSON handling\n",
        "from supabase import create_client  # Import Supabase client for database operations\n",
        "\n",
        "# Function to initialize Supabase connection\n",
        "def initialize_supabase():\n",
        "    url = os.getenv(\"SUPABASE_URL\")  # Get Supabase URL from environment variable\n",
        "    key = os.getenv(\"SUPABASE_KEY\")  # Get Supabase key from environment variable\n",
        "    client = create_client(url, key)  # Create Supabase client\n",
        "    return client\n",
        "\n",
        "# Initialize Supabase client\n",
        "client = initialize_supabase()\n",
        "\n",
        "# Define headers to match Supabase schema\n",
        "HEADERS = [\n",
        "    \"record_id\", \"created_at\", \"active\", \"date\", \"company_name\", \"symbol\", \"open\", \"high\", \"low\", \"close\", \"adj_close\", \"volume\",\n",
        "    \"dividends\", \"stock_splits\", \"dividend_yield\", \"return_on_equity\", \"earnings_per_share_eps\", \"price_to_earnings_ratio\",\n",
        "    \"market_capitalization\", \"revenue\", \"total_assets\", \"total_liabilities\", \"total_equity\", \"operating_cash_flow\",\n",
        "    \"investing_cash_flow\", \"sma_50\", \"volatility\", \"ema\", \"model_trained\"\n",
        "]\n",
        "\n",
        "# Function to fetch stock data\n",
        "def fetch_stock_data(symbols, start_date, end_date):\n",
        "    data = {}  # Initialize dictionary to store data for each symbol\n",
        "    for symbol in symbols:\n",
        "        try:\n",
        "            stock_data = yf.download(symbol, start=start_date, end=end_date)  # Fetch historical data\n",
        "            if not stock_data.empty:\n",
        "                stock_data.reset_index(inplace=True)  # Reset index to use default integer index\n",
        "                stock_data['symbol'] = symbol  # Add symbol column\n",
        "                data[symbol] = stock_data  # Store the data\n",
        "            else:\n",
        "                logging.warning(f\"No data fetched for {symbol}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error fetching data for {symbol}: {e}\")\n",
        "    return data\n",
        "\n",
        "# Function to ensure correct format for timestamp columns\n",
        "def format_timestamps(data):\n",
        "    if 'created_at' in data.columns and data['created_at'].dtype == 'datetime64[ns]':\n",
        "        data['created_at'] = data['created_at'].apply(lambda x: x.isoformat() if pd.notnull(x) else None)\n",
        "    return data\n",
        "\n",
        "# Function to serialize complex fields to JSON\n",
        "def serialize_complex_fields(data):\n",
        "    for column in data.columns:\n",
        "        if isinstance(data[column].iloc[0], (dict, list)):\n",
        "            data[column] = data[column].apply(lambda x: json.dumps(x) if pd.notnull(x) else None)\n",
        "    return data\n",
        "\n",
        "# Function to validate and store data in Supabase\n",
        "def validate_and_store_data(client, data, company_df):\n",
        "    for symbol, stock_data in data.items():\n",
        "        if not stock_data.empty:\n",
        "            # Add company name from the company DataFrame\n",
        "            stock_data['company_name'] = company_df[company_df['symbol'] == symbol]['company_name'].iloc[0]\n",
        "\n",
        "            # Add missing columns with default values\n",
        "            stock_data['record_id'] = None\n",
        "            stock_data['created_at'] = pd.Timestamp.utcnow().isoformat()\n",
        "            stock_data['active'] = True\n",
        "            stock_data['stock_splits'] = 0\n",
        "            stock_data['dividend_yield'] = stock_data.get('dividends', 0) / stock_data.get('close', 1)\n",
        "            stock_data['return_on_equity'] = 0\n",
        "            stock_data['earnings_per_share_eps'] = 0\n",
        "            stock_data['price_to_earnings_ratio'] = 0\n",
        "            stock_data['market_capitalization'] = 0\n",
        "            stock_data['revenue'] = 0\n",
        "            stock_data['total_assets'] = 0\n",
        "            stock_data['total_liabilities'] = 0\n",
        "            stock_data['total_equity'] = stock_data['total_assets'] - stock_data['total_liabilities']\n",
        "            stock_data['operating_cash_flow'] = 0\n",
        "            stock_data['investing_cash_flow'] = 0\n",
        "            stock_data['sma_50'] = stock_data['close'].rolling(window=50).mean()\n",
        "            stock_data['volatility'] = stock_data['close'].rolling(window=50).std()\n",
        "            stock_data['ema'] = stock_data['close'].ewm(span=20, adjust=False).mean()\n",
        "            stock_data['model_trained'] = False\n",
        "\n",
        "            # Ensure all columns are present and in the correct order\n",
        "            stock_data = stock_data.reindex(columns=HEADERS, fill_value=0)\n",
        "\n",
        "            # Replace NaN values with None\n",
        "            stock_data = stock_data.where(pd.notnull(stock_data), None)\n",
        "\n",
        "            # Ensure correct format for timestamp columns\n",
        "            stock_data = format_timestamps(stock_data)\n",
        "\n",
        "            # Serialize complex fields to JSON\n",
        "            stock_data = serialize_complex_fields(stock_data)\n",
        "\n",
        "            # Convert boolean columns to integers (Supabase does not support boolean values)\n",
        "            boolean_columns = stock_data.select_dtypes(include=[bool]).columns.tolist()\n",
        "            for col in boolean_columns:\n",
        "                stock_data[col] = stock_data[col].astype(int)\n",
        "\n",
        "            # Handle NaN values in critical columns\n",
        "            critical_columns = ['date', 'adj_close', 'sma_50', 'volatility']\n",
        "            for col in critical_columns:\n",
        "                if col in stock_data.columns:\n",
        "                    stock_data[col].fillna(0, inplace=True)  # Or use another appropriate value\n",
        "                    stock_data[col] = stock_data[col].apply(lambda x: 0 if pd.isnull(x) else x)  # Ensure no NaNs\n",
        "\n",
        "            # Replace invalid dates and convert to ISO format\n",
        "            stock_data['date'] = pd.to_datetime(stock_data['date'], errors='coerce')\n",
        "            stock_data['date'].fillna(pd.Timestamp(\"1970-01-01\"), inplace=True)\n",
        "            stock_data['date'] = stock_data['date'].apply(lambda x: x.isoformat() if isinstance(x, pd.Timestamp) else x)\n",
        "\n",
        "            # Insert data into Supabase\n",
        "            try:\n",
        "                client.table('Historical Stock Dividend Data').upsert(stock_data.to_dict(orient='records')).execute()\n",
        "                logging.info(f\"Data for {symbol} stored successfully in Supabase.\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error storing data for {symbol} in Supabase: {e}\")\n",
        "        else:\n",
        "            logging.warning(f\"No data fetched for {symbol}\")\n",
        "\n",
        "# Example usage\n",
        "response = client.table(\"Stock Company Information\").select(\"*\").execute()\n",
        "company_df = pd.DataFrame(response.data)  # Convert response data to DataFrame\n",
        "symbols = company_df['symbol'].tolist()  # Get list of symbols\n",
        "\n",
        "# Fetch and validate data, then store it in Supabase\n",
        "stock_data_batch = fetch_stock_data(symbols, \"2018-01-01\", None)\n",
        "validate_and_store_data(client, stock_data_batch, company_df)\n"
      ],
      "metadata": {
        "id": "oe168oZNmof2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Incremental Data Updates**"
      ],
      "metadata": {
        "id": "1BwfLEq8mhkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime  # Import datetime for date manipulation\n",
        "import pandas as pd  # Import pandas for data manipulation\n",
        "from supabase import create_client  # Import Supabase client for database operations\n",
        "import logging  # Import logging for logging messages\n",
        "import yfinance as yf  # Import the yfinance library for fetching stock data\n",
        "import os  # Import os for environment variable handling\n",
        "import json  # Import json for JSON handling\n",
        "\n",
        "# Set up logging configuration\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[\n",
        "    logging.FileHandler(\"stock_data_update.log\"),  # Log to a file\n",
        "    logging.StreamHandler()  # Log to the console\n",
        "])\n",
        "\n",
        "# Set environment variables for Supabase connection\n",
        "os.environ[\"SUPABASE_URL\"] = \"https://zphgtxuwecitedpxrahe.supabase.co\"\n",
        "os.environ[\"SUPABASE_KEY\"] = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InpwaGd0eHV3ZWNpdGVkcHhyYWhlIiwicm9sZSI6ImFub24iLCJpYXQiOjE3MTkzNTg3NjMsImV4cCI6MjAzNDkzNDc2M30.oAtpm0gYUEihPSkXtvpf0v_mu6H3lYqUYfcQVt-tI2w\"  # Replace with your actual Supabase anon key\n",
        "\n",
        "# Initialize Supabase client\n",
        "supabase_url = os.getenv(\"SUPABASE_URL\")  # Get Supabase URL from environment variable\n",
        "supabase_key = os.getenv(\"SUPABASE_KEY\")  # Get Supabase key from environment variable\n",
        "client = create_client(supabase_url, supabase_key)  # Create Supabase client\n",
        "\n",
        "# Function to fetch incremental stock data\n",
        "def fetch_incremental_stock_data(symbol, last_date):\n",
        "    logging.info(f\"Fetching incremental data for {symbol} from {last_date}\")\n",
        "    start_date = (pd.to_datetime(last_date) + pd.DateOffset(1)).strftime('%Y-%m-%d')  # Calculate the start date\n",
        "    end_date = datetime.datetime.now().strftime('%Y-%m-%d')  # Get the current date\n",
        "    stock_data = yf.download(symbol, start=start_date, end=end_date)  # Fetch stock data\n",
        "    return stock_data\n",
        "\n",
        "# Function to check data quality\n",
        "def check_data_quality(df, symbol):\n",
        "    if df.isnull().values.any():  # Check for null values\n",
        "        logging.warning(f\"Data for {symbol} contains null values.\")\n",
        "        df = df.dropna()  # Drop rows with null values\n",
        "\n",
        "    if df.duplicated().any():  # Check for duplicate rows\n",
        "        logging.warning(f\"Data for {symbol} contains duplicate rows.\")\n",
        "        df = df.drop_duplicates()  # Drop duplicate rows\n",
        "\n",
        "    return df\n",
        "\n",
        "# Function to add features to the stock data\n",
        "def add_features(data, symbol, info):\n",
        "    data.columns = data.columns.str.lower()  # Convert column names to lowercase\n",
        "    data['symbol'] = symbol  # Add symbol column\n",
        "    data['company_name'] = info.get('longName', 'N/A')  # Add company name column\n",
        "    data['active'] = True  # Mark as active by default\n",
        "    data['dividends'] = info.get('trailingAnnualDividendYield', 0)  # Add dividends column\n",
        "    data['stock_splits'] = info.get('lastSplitFactor', 0)  # Add stock splits column\n",
        "    data['dividend_yield'] = info.get('dividendYield', 0)  # Add dividend yield column\n",
        "    data['payout_ratio'] = info.get('payoutRatio', 0)  # Add payout ratio column\n",
        "    data['free_cash_flow'] = info.get('freeCashflow', 0)  # Add free cash flow column\n",
        "    data['return_on_equity'] = info.get('returnOnEquity', 0)  # Add return on equity column\n",
        "    data['earnings_per_share_eps'] = info.get('trailingEps', 0)  # Add earnings per share column\n",
        "    data['price_to_earnings_ratio'] = info.get('trailingPE', 0)  # Add price-to-earnings ratio column\n",
        "    data['market_capitalization'] = info.get('marketCap', 0)  # Add market capitalization column\n",
        "    data['revenue'] = info.get('totalRevenue', 0)  # Add revenue column\n",
        "    data['net_income'] = info.get('netIncomeToCommon', 0)  # Add net income column\n",
        "    data['total_assets'] = info.get('totalAssets', 0)  # Add total assets column\n",
        "    data['total_liabilities'] = info.get('totalLiab', 0)  # Add total liabilities column\n",
        "    data['total_equity'] = data['total_assets'] - data['total_liabilities']  # Calculate total equity\n",
        "    data['operating_cash_flow'] = info.get('operatingCashflow', 0)  # Add operating cash flow column\n",
        "    data['investing_cash_flow'] = info.get('investingCashflow', 0)  # Add investing cash flow column\n",
        "    data['financing_cash_flow'] = info.get('financingCashflow', 0)  # Add financing cash flow column\n",
        "    data['sma_50'] = data['close'].rolling(window=50).mean()  # Calculate 50-day SMA\n",
        "    data['volatility'] = data['close'].rolling(window=50).std()  # Calculate 50-day volatility\n",
        "    data['ema'] = data['close'].ewm(span=20, adjust=False).mean()  # Calculate 20-day EMA\n",
        "    data['model_trained'] = False  # Set model trained status to False\n",
        "    data['created_at'] = pd.Timestamp.utcnow().isoformat()  # Add created at timestamp\n",
        "    return data\n",
        "\n",
        "# Function to update Supabase with incremental data\n",
        "def update_supabase_with_incremental_data(client, company_df):\n",
        "    existing_data = client.table(\"Historical Stock Dividend Data\").select(\"*\").execute().data  # Fetch existing data from Supabase\n",
        "    existing_df = pd.DataFrame(existing_data)  # Convert to DataFrame\n",
        "\n",
        "    symbols = company_df['symbol'].tolist()  # Get list of symbols\n",
        "\n",
        "    for symbol in symbols:\n",
        "        try:\n",
        "            last_date = existing_df[existing_df['symbol'] == symbol]['date'].max()  # Get the last date of existing data for the symbol\n",
        "            if pd.notnull(last_date):\n",
        "                incremental_data = fetch_incremental_stock_data(symbol, last_date)  # Fetch incremental data\n",
        "                if not incremental_data.empty:\n",
        "                    incremental_data = check_data_quality(incremental_data, symbol)  # Check data quality\n",
        "\n",
        "                    incremental_data.reset_index(inplace=True)  # Reset index to use default integer index\n",
        "                    incremental_data['symbol'] = symbol  # Add symbol column\n",
        "                    incremental_data['company_name'] = company_df[company_df['symbol'] == symbol]['company_name'].iloc[0]  # Add company name\n",
        "\n",
        "                    incremental_data['dividends'] = incremental_data.get('dividends', 0)  # Add dividends column\n",
        "                    incremental_data['dividend_yield'] = incremental_data['dividends'] / incremental_data['close'] if 'dividends' in incremental_data.columns and incremental_data['close'] != 0 else 0  # Calculate dividend yield\n",
        "\n",
        "                    # Define the headers\n",
        "                    headers = [\n",
        "                        \"record_id\", \"created_at\", \"active\", \"date\", \"company_name\", \"symbol\", \"open\", \"high\", \"low\", \"close\", \"adj_close\", \"volume\",\n",
        "                        \"dividends\", \"stock_splits\", \"dividend_yield\", \"return_on_equity\", \"earnings_per_share_eps\", \"price_to_earnings_ratio\",\n",
        "                        \"market_capitalization\", \"revenue\", \"total_assets\", \"total_liabilities\", \"total_equity\", \"operating_cash_flow\",\n",
        "                        \"investing_cash_flow\", \"sma_50\", \"volatility\", \"ema\", \"model_trained\"\n",
        "                    ]\n",
        "\n",
        "                    # Placeholder values for missing columns\n",
        "                    for col in headers:\n",
        "                        if col not in incremental_data.columns:\n",
        "                            incremental_data[col] = None\n",
        "\n",
        "                    # Reindex columns to match the headers\n",
        "                    incremental_data = incremental_data.reindex(columns=headers, fill_value=None)\n",
        "\n",
        "                    # Replace NaN values with None\n",
        "                    incremental_data = incremental_data.where(pd.notnull(incremental_data), None)\n",
        "\n",
        "                    # Ensure correct format for timestamp columns\n",
        "                    if 'created_at' in incremental_data.columns and incremental_data['created_at'].dtype == 'datetime64[ns]':\n",
        "                        incremental_data['created_at'] = incremental_data['created_at'].apply(lambda x: x.isoformat() if pd.notnull(x) else None)\n",
        "\n",
        "                    # Convert boolean columns to integers (Supabase does not support boolean values)\n",
        "                    boolean_columns = incremental_data.select_dtypes(include=[bool]).columns.tolist()\n",
        "                    for col in boolean_columns:\n",
        "                        incremental_data[col] = incremental_data[col].astype(int)\n",
        "\n",
        "                    # Handle NaN values in critical columns\n",
        "                    critical_columns = ['date', 'adj_close', 'sma_50', 'volatility']\n",
        "                    for col in critical_columns:\n",
        "                        if col in incremental_data.columns:\n",
        "                            incremental_data[col].fillna(0, inplace=True)  # Or use another appropriate value\n",
        "                            incremental_data[col] = incremental_data[col].apply(lambda x: 0 if pd.isnull(x) else x)  # Ensure no NaNs\n",
        "\n",
        "                    # Log request data before sending to Supabase\n",
        "                    logging.info(f\"Request Data for {symbol}: {incremental_data.to_dict(orient='records')}\")\n",
        "\n",
        "                    # Convert DataFrame to dictionary and insert into Supabase\n",
        "                    stock_data_dict = incremental_data.to_dict(orient='records')\n",
        "                    for record in stock_data_dict:\n",
        "                        client.table(\"Historical Stock Dividend Data\").upsert(record).execute()\n",
        "                    logging.info(f\"Successfully updated data for {symbol}\")\n",
        "                else:\n",
        "                    logging.warning(f\"No incremental data fetched for {symbol}\")\n",
        "            else:\n",
        "                logging.warning(f\"No existing data for {symbol}, skipping incremental update\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error updating data for {symbol}: {e}\")\n",
        "\n",
        "# Usage\n",
        "# Fetch company_df from your data source, e.g., Supabase\n",
        "update_supabase_with_incremental_data(client, company_df)\n"
      ],
      "metadata": {
        "id": "GVFInRcclzd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parallel Processing**"
      ],
      "metadata": {
        "id": "lG4jM89_EQcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time  # Import time for sleep function\n",
        "import pandas as pd  # Import pandas for data manipulation\n",
        "import yfinance as yf  # Import the yfinance library for fetching stock data\n",
        "from concurrent.futures import ThreadPoolExecutor  # Import ThreadPoolExecutor for parallel processing\n",
        "from supabase import create_client, Client  # Import Supabase client for database operations\n",
        "import logging  # Import logging for logging messages\n",
        "import os  # Import os for environment variable handling\n",
        "import json  # Import json for JSON handling\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Set environment variables for Supabase connection\n",
        "os.environ[\"SUPABASE_URL\"] = \"https://zphgtxuwecitedpxrahe.supabase.co\"\n",
        "os.environ[\"SUPABASE_KEY\"] = \"your_supabase_anon_key\"  # Replace with your actual Supabase anon key\n",
        "\n",
        "# Initialize Supabase connection\n",
        "url = os.getenv(\"SUPABASE_URL\")  # Get Supabase URL from environment variable\n",
        "key = os.getenv(\"SUPABASE_KEY\")  # Get Supabase key from environment variable\n",
        "supabase: Client = create_client(url, key)  # Create Supabase client\n",
        "\n",
        "# Constants for file paths and batch size\n",
        "PROGRESS_FILE = 'progress.csv'\n",
        "BATCH_SIZE = 10\n",
        "START_DATE = \"2018-01-01\"\n",
        "END_DATE = None  # Fetch data up to the current date\n",
        "\n",
        "# Function to add features to the stock data\n",
        "def add_features(data, symbol, info):\n",
        "    data.columns = data.columns.str.lower()  # Convert column names to lowercase\n",
        "    data['symbol'] = symbol  # Add symbol column\n",
        "    data['company_name'] = info.get('longName', 'N/A')  # Add company name column\n",
        "    data['active'] = True  # Mark as active by default\n",
        "    data['dividends'] = info.get('trailingAnnualDividendYield', 0)  # Add dividends column\n",
        "    data['stock_splits'] = info.get('lastSplitFactor', 0)  # Add stock splits column\n",
        "    data['dividend_yield'] = info.get('dividendYield', 0)  # Add dividend yield column\n",
        "    data['payout_ratio'] = info.get('payoutRatio', 0)  # Add payout ratio column\n",
        "    data['free_cash_flow'] = info.get('freeCashflow', 0)  # Add free cash flow column\n",
        "    data['return_on_equity'] = info.get('returnOnEquity', 0)  # Add return on equity column\n",
        "    data['earnings_per_share_eps'] = info.get('trailingEps', 0)  # Add earnings per share column\n",
        "    data['price_to_earnings_ratio'] = info.get('trailingPE', 0)  # Add price-to-earnings ratio column\n",
        "    data['market_capitalization'] = info.get('marketCap', 0)  # Add market capitalization column\n",
        "    data['revenue'] = info.get('totalRevenue', 0)  # Add revenue column\n",
        "    data['net_income'] = info.get('netIncomeToCommon', 0)  # Add net income column\n",
        "    data['total_assets'] = info.get('totalAssets', 0)  # Add total assets column\n",
        "    data['total_liabilities'] = info.get('totalLiab', 0)  # Add total liabilities column\n",
        "    data['total_equity'] = data['total_assets'] - data['total_liabilities']  # Calculate total equity\n",
        "    data['operating_cash_flow'] = info.get('operatingCashflow', 0)  # Add operating cash flow column\n",
        "    data['investing_cash_flow'] = info.get('investingCashflow', 0)  # Add investing cash flow column\n",
        "    data['financing_cash_flow'] = info.get('financingCashflow', 0)  # Add financing cash flow column\n",
        "    data['sma_50'] = data['close'].rolling(window=50).mean()  # Calculate 50-day SMA\n",
        "    data['volatility'] = data['close'].rolling(window=50).std()  # Calculate 50-day volatility\n",
        "    data['ema'] = data['close'].ewm(span=20, adjust=False).mean()  # Calculate 20-day EMA\n",
        "    data['model_trained'] = False  # Set model trained status to False\n",
        "    data['created_at'] = pd.Timestamp.utcnow().isoformat()  # Add created at timestamp\n",
        "    data['record_id'] = None  # Add record ID column\n",
        "    return data\n",
        "\n",
        "# Function to check stock status\n",
        "def check_stock_status(symbol):\n",
        "    try:\n",
        "        stock = yf.Ticker(symbol)  # Create a Ticker object\n",
        "        info = stock.info  # Fetch stock info\n",
        "        if 'quoteType' in info and info['quoteType'] in ['EQUITY', 'ETF']:  # Check if the stock is active\n",
        "            logging.info(f\"{symbol} is active with quoteType: {info['quoteType']}\")\n",
        "            return True, info\n",
        "        else:\n",
        "            logging.warning(f\"{symbol} might be delisted or have no market data available.\")\n",
        "            return False, None\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error checking status for {symbol}: {e}\")\n",
        "        return False, None\n",
        "\n",
        "# Function to fetch stock data\n",
        "def fetch_stock_data(symbol):\n",
        "    valid, info = check_stock_status(symbol)  # Check if the stock is valid\n",
        "    if valid:\n",
        "        try:\n",
        "            stock_data = yf.download(symbol, start=START_DATE, end=END_DATE)  # Fetch historical data\n",
        "            if not stock_data.empty:\n",
        "                stock_data = add_features(stock_data, symbol, info)  # Add features to the data\n",
        "                stock_data.index = pd.to_datetime(stock_data.index)  # Ensure date column is a datetime object\n",
        "                stock_data.reset_index(inplace=True)  # Reset index to use default integer index\n",
        "                logging.info(f\"Fetched and added features for {symbol}\")\n",
        "                return stock_data\n",
        "            else:\n",
        "                logging.warning(f\"No data fetched for {symbol}\")\n",
        "                return pd.DataFrame()\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error fetching data for {symbol}: {e}\")\n",
        "            return pd.DataFrame()\n",
        "    else:\n",
        "        # Create an empty DataFrame with predefined columns if the stock symbol is invalid\n",
        "        stock_data = pd.DataFrame(columns=[\n",
        "            \"date\", \"symbol\", \"company_name\", \"open\", \"high\", \"low\", \"adj_close\", \"volume\",\n",
        "            \"dividends\", \"stock_splits\", \"dividend_yield\", \"return_on_equity\", \"earnings_per_share_eps\",\n",
        "            \"price_to_earnings_ratio\", \"market_capitalization\", \"revenue\", \"net_income\", \"total_assets\",\n",
        "            \"total_liabilities\", \"total_equity\", \"operating_cash_flow\", \"investing_cash_flow\",\n",
        "            \"financing_cash_flow\", \"sma_50\", \"volatility\", \"ema\", \"active\", \"created_at\", \"record_id\", \"model_trained\", \"close\"\n",
        "        ])\n",
        "        stock_data.loc[0] = [None] * (len(stock_data.columns) - 1) + [False]  # Mark as inactive\n",
        "        logging.info(f\"Skipping {symbol} due to validation issues.\")\n",
        "        return stock_data\n",
        "\n",
        "# Function to load progress from a file\n",
        "def load_progress(progress_file):\n",
        "    if os.path.exists(progress_file):\n",
        "        return pd.read_csv(progress_file)['symbol'].tolist()  # Load processed symbols\n",
        "    return []\n",
        "\n",
        "# Function to save progress to a file\n",
        "def save_progress(progress_file, processed_symbols):\n",
        "    pd.DataFrame({'symbol': processed_symbols}).to_csv(progress_file, index=False)  # Save processed symbols\n",
        "\n",
        "# Function to validate data and store it in Supabase\n",
        "def validate_and_store_data(client, data, company_df):\n",
        "    # Define the headers\n",
        "    headers = [\n",
        "        \"date\", \"symbol\", \"company_name\", \"open\", \"high\", \"low\", \"close\", \"adj_close\", \"volume\",\n",
        "        \"dividends\", \"stock_splits\", \"dividend_yield\", \"return_on_equity\", \"earnings_per_share_eps\",\n",
        "        \"price_to_earnings_ratio\", \"market_capitalization\", \"revenue\", \"net_income\", \"total_assets\",\n",
        "        \"total_liabilities\", \"total_equity\", \"operating_cash_flow\", \"investing_cash_flow\", \"financing_cash_flow\",\n",
        "        \"sma_50\", \"volatility\", \"ema\", \"active\", \"model_trained\", \"created_at\", \"record_id\"\n",
        "    ]\n",
        "\n",
        "    for symbol, stock_data in data.items():\n",
        "        if not stock_data.empty:\n",
        "            stock_data.reset_index(drop=True, inplace=True)  # Reset index to use default integer index\n",
        "            stock_data['symbol'] = symbol  # Add symbol column\n",
        "            stock_data['company_name'] = company_df[company_df['symbol'] == symbol]['company_name'].iloc[0]  # Add company name\n",
        "\n",
        "            # Placeholder values for missing columns\n",
        "            for col in headers:\n",
        "                if col not in stock_data.columns:\n",
        "                    stock_data[col] = None\n",
        "\n",
        "            # Reindex columns to match the headers\n",
        "            stock_data = stock_data.reindex(columns=headers, fill_value=None)\n",
        "\n",
        "            # Replace out-of-range float values and NaNs\n",
        "            stock_data.replace([float('inf'), float('-inf')], None, inplace=True)\n",
        "            stock_data = stock_data.where(pd.notnull(stock_data), None)\n",
        "\n",
        "            # Ensure correct format for timestamp columns\n",
        "            if 'created_at' in stock_data.columns and stock_data['created_at'].dtype == 'datetime64[ns]':\n",
        "                stock_data['created_at'] = stock_data['created_at'].apply(lambda x: x.isoformat() if pd.notnull(x) else None)\n",
        "\n",
        "            # Convert boolean columns to integers (Supabase does not support boolean values)\n",
        "            boolean_columns = stock_data.select_dtypes(include=[bool]).columns.tolist()\n",
        "            for col in boolean_columns:\n",
        "                stock_data[col] = stock_data[col].astype(int)\n",
        "\n",
        "            # Handle NaN values in critical columns\n",
        "            critical_columns = ['date', 'adj_close', 'sma_50', 'volatility']\n",
        "            for col in critical_columns:\n",
        "                if col in stock_data.columns:\n",
        "                    stock_data[col].fillna(0, inplace=True)  # Or use another appropriate value\n",
        "                    stock_data[col] = stock_data[col].apply(lambda x: 0 if pd.isnull(x) else x)  # Ensure no NaNs\n",
        "\n",
        "            # Log request data before sending to Supabase\n",
        "            logging.info(f\"Request Data for {symbol}: {stock_data.to_dict(orient='records')}\")\n",
        "\n",
        "            # Convert DataFrame to dictionary and insert into Supabase\n",
        "            stock_data_dict = stock_data.to_dict(orient='records')\n",
        "            for record in stock_data_dict:\n",
        "                client.table('Historical Stock Dividend Data').upsert(record).execute()\n",
        "            logging.info(f\"Data for {symbol} stored successfully in Supabase.\")\n",
        "        else:\n",
        "            logging.warning(f\"No data fetched for {symbol}\")\n",
        "\n",
        "# Fetch company data from Supabase\n",
        "response = supabase.table(\"Stock Company Information\").select(\"*\").execute()\n",
        "company_df = pd.DataFrame(response.data)  # Convert response data to DataFrame\n",
        "symbols = company_df['symbol'].tolist()  # Get list of symbols\n",
        "\n",
        "# Load progress\n",
        "processed_symbols = load_progress(PROGRESS_FILE)  # Load progress from file\n",
        "symbols_to_process = [symbol for symbol in symbols if symbol not in processed_symbols]  # Get symbols to process\n",
        "\n",
        "# Parallel processing using ThreadPoolExecutor\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    stock_data_list = list(executor.map(fetch_stock_data, symbols_to_process))  # Fetch stock data in parallel\n",
        "\n",
        "# Validate and store data\n",
        "for stock_data in stock_data_list:\n",
        "    if not stock_data.empty:\n",
        "        symbol = stock_data['symbol'].iloc[0]  # Get symbol from the stock data\n",
        "        validate_and_store_data(supabase, {symbol: stock_data}, company_df)  # Validate and store data\n",
        "        processed_symbols.append(symbol)  # Add symbol to processed symbols\n",
        "        save_progress(PROGRESS_FILE, processed_symbols)  # Save progress\n",
        "    else:\n",
        "        logging.warning(f\"No data fetched for {symbol}\")\n"
      ],
      "metadata": {
        "id": "dpHWOU8HFoOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logging and Monitoring**"
      ],
      "metadata": {
        "id": "45rdqud4HsVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os  # Import os for environment variable handling\n",
        "import logging  # Import logging for logging messages\n",
        "import pandas as pd  # Import pandas for data manipulation\n",
        "import datetime  # Import datetime for date manipulation\n",
        "from retrying import retry  # Import retrying for retry mechanism\n",
        "import smtplib  # Import smtplib for sending emails\n",
        "from email.mime.text import MIMEText  # Import MIMEText for email text handling\n",
        "from email.mime.multipart import MIMEMultipart  # Import MIMEMultipart for email multipart handling\n",
        "from supabase import create_client  # Import Supabase client for database operations\n",
        "import yfinance as yf  # Import yfinance for fetching stock data\n",
        "import json  # Import json for JSON handling\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['SUPABASE_URL'] = 'https://zphgtxuwecitedpxrahe.supabase.co'\n",
        "os.environ['SUPABASE_KEY'] = 'your_supabase_key'\n",
        "os.environ['SENDER_EMAIL'] = 'markallenacosta@gmail.com'\n",
        "os.environ['RECEIVER_EMAIL'] = 'markallenacosta@gmail.com'\n",
        "os.environ['EMAIL_PASSWORD'] = 'cap5000ACXy2!'\n",
        "\n",
        "# Set up logging configuration\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[\n",
        "    logging.FileHandler(\"model_training.log\"),  # Log to a file\n",
        "    logging.StreamHandler()  # Log to the console\n",
        "])\n",
        "\n",
        "# Initialize Supabase client\n",
        "supabase_url = os.getenv(\"SUPABASE_URL\")  # Get Supabase URL from environment variable\n",
        "supabase_key = os.getenv(\"SUPABASE_KEY\")  # Get Supabase key from environment variable\n",
        "client = create_client(supabase_url, supabase_key)  # Create Supabase client\n",
        "\n",
        "# Email details for sending alerts\n",
        "sender_email = os.getenv(\"SENDER_EMAIL\")  # Get sender email from environment variable\n",
        "receiver_email = os.getenv(\"RECEIVER_EMAIL\")  # Get receiver email from environment variable\n",
        "password = os.getenv(\"EMAIL_PASSWORD\")  # Get email password from environment variable\n",
        "\n",
        "# Function to send email alerts\n",
        "def send_alert(message):\n",
        "    subject = \"Stock Trading App Alert\"\n",
        "\n",
        "    msg = MIMEMultipart()\n",
        "    msg['From'] = sender_email\n",
        "    msg['To'] = receiver_email\n",
        "    msg['Subject'] = subject\n",
        "    msg.attach(MIMEText(message, 'plain'))\n",
        "\n",
        "    try:\n",
        "        server = smtplib.SMTP('smtp.gmail.com', 587)\n",
        "        server.starttls()\n",
        "        server.login(sender_email, password)\n",
        "        text = msg.as_string()\n",
        "        server.sendmail(sender_email, receiver_email, text)\n",
        "        server.quit()\n",
        "        logging.info(\"Alert email sent successfully.\")\n",
        "    except smtplib.SMTPException as e:\n",
        "        logging.error(f\"Failed to send alert email: {e}\")\n",
        "\n",
        "# Retry configuration for fetching data\n",
        "@retry(wait_fixed=2000, stop_max_attempt_number=5)\n",
        "def fetch_incremental_stock_data(symbol, last_date):\n",
        "    logging.info(f\"Fetching incremental data for {symbol} from {last_date}\")\n",
        "    start_date = (pd.to_datetime(last_date) + pd.DateOffset(1)).strftime('%Y-%m-%d')\n",
        "    end_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
        "    stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
        "    if stock_data.empty:\n",
        "        raise ValueError(f\"No data fetched for {symbol}\")\n",
        "    return stock_data\n",
        "\n",
        "# Function to check data quality\n",
        "def check_data_quality(df, symbol):\n",
        "    if df.isnull().values.any():  # Check for null values\n",
        "        logging.warning(f\"Data for {symbol} contains null values.\")\n",
        "        df = df.dropna()  # Drop rows with null values\n",
        "\n",
        "    if df.duplicated().any():  # Check for duplicate rows\n",
        "        logging.warning(f\"Data for {symbol} contains duplicate rows.\")\n",
        "        df = df.drop_duplicates()  # Drop duplicate rows\n",
        "\n",
        "    return df\n",
        "\n",
        "# Function to ensure correct format for timestamp columns\n",
        "def format_timestamps(data):\n",
        "    if 'created_at' in data.columns and data['created_at'].dtype == 'datetime64[ns]':\n",
        "        data['created_at'] = data['created_at'].apply(lambda x: x.isoformat() if pd.notnull(x) else None)\n",
        "    return data\n",
        "\n",
        "# Function to serialize complex fields to JSON\n",
        "def serialize_complex_fields(data):\n",
        "    for column in data.columns:\n",
        "        if isinstance(data[column].iloc[0], (dict, list)):\n",
        "            data[column] = data[column].apply(lambda x: json.dumps(x) if pd.notnull(x) else None)\n",
        "    return data\n",
        "\n",
        "# Function to update Supabase with incremental data\n",
        "def update_supabase_with_incremental_data(client, company_df):\n",
        "    existing_data = client.table(\"Historical Stock Dividend Data\").select(\"*\").execute().data  # Fetch existing data from Supabase\n",
        "    existing_df = pd.DataFrame(existing_data)  # Convert to DataFrame\n",
        "\n",
        "    symbols = company_df['symbol'].tolist()  # Get list of symbols\n",
        "\n",
        "    for symbol in symbols:\n",
        "        try:\n",
        "            last_date = existing_df[existing_df['symbol'] == symbol]['date'].max()  # Get the last date of existing data for the symbol\n",
        "            if pd.notnull(last_date):\n",
        "                incremental_data = fetch_incremental_stock_data(symbol, last_date)  # Fetch incremental data\n",
        "                if not incremental_data.empty:\n",
        "                    incremental_data = check_data_quality(incremental_data, symbol)  # Check data quality\n",
        "\n",
        "                    incremental_data.reset_index(inplace=True)  # Reset index to use default integer index\n",
        "                    incremental_data['symbol'] = symbol  # Add symbol column\n",
        "                    incremental_data['company_name'] = company_df[company_df['symbol'] == symbol]['company_name'].iloc[0]  # Add company name\n",
        "\n",
        "                    incremental_data['dividends'] = incremental_data.get('dividends', 0)  # Add dividends column\n",
        "                    incremental_data['dividend_yield'] = incremental_data['dividends'] / incremental_data['close'] if 'dividends' in incremental_data.columns and incremental_data['close'] != 0 else 0  # Calculate dividend yield\n",
        "\n",
        "                    # Define the headers\n",
        "                    headers = [\n",
        "                        \"record_id\", \"created_at\", \"active\", \"date\", \"company_name\", \"symbol\", \"open\", \"high\", \"low\", \"close\",\n",
        "                        \"adj_close\", \"volume\", \"dividends\", \"stock_splits\", \"dividend_yield\", \"return_on_equity\",\n",
        "                        \"earnings_per_share_eps\", \"price_to_earnings_ratio\", \"market_capitalization\", \"revenue\",\n",
        "                        \"net_income\", \"total_assets\", \"total_liabilities\", \"total_equity\", \"operating_cash_flow\",\n",
        "                        \"investing_cash_flow\", \"sma_50\", \"volatility\", \"ema\", \"model_trained\"\n",
        "                    ]\n",
        "\n",
        "                    # Placeholder values for missing columns\n",
        "                    for col in headers:\n",
        "                        if col not in incremental_data.columns:\n",
        "                            incremental_data[col] = None\n",
        "\n",
        "                    # Reindex columns to match the headers\n",
        "                    incremental_data = incremental_data.reindex(columns=headers, fill_value=None)\n",
        "\n",
        "                    # Replace out-of-range float values and NaNs\n",
        "                    incremental_data.replace([float('inf'), float('-inf')], None, inplace=True)\n",
        "                    incremental_data = incremental_data.where(pd.notnull(incremental_data), None)\n",
        "\n",
        "                    # Ensure correct format for timestamp columns\n",
        "                    incremental_data = format_timestamps(incremental_data)\n",
        "\n",
        "                    # Serialize complex fields to JSON\n",
        "                    incremental_data = serialize_complex_fields(incremental_data)\n",
        "\n",
        "                    # Convert boolean columns to integers (Supabase does not support boolean values)\n",
        "                    boolean_columns = incremental_data.select_dtypes(include=[bool]).columns.tolist()\n",
        "                    for col in boolean_columns:\n",
        "                        incremental_data[col] = incremental_data[col].astype(int)\n",
        "\n",
        "                    # Handle NaN values in critical columns\n",
        "                    critical_columns = ['date', 'adj_close', 'sma_50', 'volatility']\n",
        "                    for col in critical_columns:\n",
        "                        if col in incremental_data.columns:\n",
        "                            incremental_data[col].fillna(0, inplace=True)  # Or use another appropriate value\n",
        "                            incremental_data[col] = incremental_data[col].apply(lambda x: 0 if pd.isnull(x) else x)  # Ensure no NaNs\n",
        "\n",
        "                    # Log request data before sending to Supabase\n",
        "                    logging.info(f\"Request Data for {symbol}: {incremental_data.to_dict(orient='records')}\")\n",
        "\n",
        "                    # Convert DataFrame to dictionary and insert into Supabase\n",
        "                    stock_data_dict = incremental_data.to_dict(orient='records')\n",
        "                    for record in stock_data_dict:\n",
        "                        client.table(\"Historical Stock Dividend Data\").upsert(record).execute()\n",
        "                    logging.info(f\"Successfully updated data for {symbol}\")\n",
        "                else:\n",
        "                    logging.warning(f\"No incremental data fetched for {symbol}\")\n",
        "            else:\n",
        "                logging.warning(f\"No existing data for {symbol}, skipping incremental update\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error updating data for {symbol}: {e}\")\n",
        "            send_alert(f\"Error updating data for {symbol}: {e}\")\n",
        "\n",
        "# Usage example\n",
        "company_df = pd.DataFrame([{\"symbol\": \"AAPL\", \"company_name\": \"Apple Inc.\"}])  # Example DataFrame setup\n",
        "update_supabase_with_incremental_data(client, company_df)\n"
      ],
      "metadata": {
        "id": "_GgKwfpDHuTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Error Handling and Retry Logic**"
      ],
      "metadata": {
        "id": "2WRSTrM6KGTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging  # Import logging for logging messages\n",
        "import pandas as pd  # Import pandas for data manipulation\n",
        "import time  # Import time for sleep function\n",
        "from retrying import retry  # Import retrying for retry mechanism\n",
        "import smtplib  # Import smtplib for sending emails\n",
        "from email.mime.text import MIMEText  # Import MIMEText for email text handling\n",
        "from email.mime.multipart import MIMEMultipart  # Import MIMEMultipart for email multipart handling\n",
        "from supabase import create_client, Client  # Import Supabase client for database operations\n",
        "import os  # Import os for environment variable handling\n",
        "import datetime  # Import datetime for date manipulation\n",
        "import yfinance as yf  # Import yfinance for fetching stock data\n",
        "import json  # Import json for JSON handling\n",
        "\n",
        "# Set up logging configuration\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[\n",
        "    logging.FileHandler(\"model_training.log\"),  # Log to a file\n",
        "    logging.StreamHandler()  # Log to the console\n",
        "])\n",
        "\n",
        "# Fetch environment variables\n",
        "SUPABASE_URL = os.getenv('SUPABASE_URL')\n",
        "SUPABASE_KEY = os.getenv('SUPABASE_KEY')\n",
        "SENDER_EMAIL = os.getenv('SENDER_EMAIL')\n",
        "RECEIVER_EMAIL = os.getenv('RECEIVER_EMAIL')\n",
        "EMAIL_PASSWORD = os.getenv('EMAIL_PASSWORD')\n",
        "\n",
        "# Initialize Supabase client\n",
        "client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "# Function to send email alerts\n",
        "def send_alert(message):\n",
        "    subject = \"Stock Trading App Alert\"\n",
        "\n",
        "    msg = MIMEMultipart()\n",
        "    msg['From'] = SENDER_EMAIL\n",
        "    msg['To'] = RECEIVER_EMAIL\n",
        "    msg['Subject'] = subject\n",
        "    msg.attach(MIMEText(message, 'plain'))\n",
        "\n",
        "    try:\n",
        "        server = smtplib.SMTP('smtp.gmail.com', 587)\n",
        "        server.starttls()\n",
        "        server.login(SENDER_EMAIL, EMAIL_PASSWORD)\n",
        "        text = msg.as_string()\n",
        "        server.sendmail(SENDER_EMAIL, RECEIVER_EMAIL, text)\n",
        "        server.quit()\n",
        "        logging.info(\"Alert email sent successfully.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to send alert email: {e}\")\n",
        "\n",
        "# Retry configuration for fetching data\n",
        "@retry(wait_fixed=2000, stop_max_attempt_number=5)\n",
        "def fetch_incremental_stock_data(symbol, last_date):\n",
        "    logging.info(f\"Fetching incremental data for {symbol} from {last_date}\")\n",
        "    start_date = (pd.to_datetime(last_date) + pd.DateOffset(1)).strftime('%Y-%m-%d')\n",
        "    end_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
        "    stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
        "    if stock_data.empty:\n",
        "        raise ValueError(f\"No data fetched for {symbol}\")\n",
        "    return stock_data\n",
        "\n",
        "# Function to check data quality\n",
        "def check_data_quality(df, symbol):\n",
        "    if df.isnull().values.any():  # Check for null values\n",
        "        logging.warning(f\"Data for {symbol} contains null values.\")\n",
        "        df = df.dropna()  # Drop rows with null values\n",
        "\n",
        "    if df.duplicated().any():  # Check for duplicate rows\n",
        "        logging.warning(f\"Data for {symbol} contains duplicate rows.\")\n",
        "        df = df.drop_duplicates()  # Drop duplicate rows\n",
        "\n",
        "    return df\n",
        "\n",
        "# Function to ensure correct format for timestamp columns\n",
        "def format_timestamps(data):\n",
        "    if 'created_at' in data.columns and data['created_at'].dtype == 'datetime64[ns]':\n",
        "        data['created_at'] = data['created_at'].apply(lambda x: x.isoformat() if pd.notnull(x) else None)\n",
        "    return data\n",
        "\n",
        "# Function to serialize complex fields to JSON\n",
        "def serialize_complex_fields(data):\n",
        "    for column in data.columns:\n",
        "        if isinstance(data[column].iloc[0], (dict, list)):\n",
        "            data[column] = data[column].apply(lambda x: json.dumps(x) if pd.notnull(x) else None)\n",
        "    return data\n",
        "\n",
        "# Function to update Supabase with incremental data\n",
        "def update_supabase_with_incremental_data(client, company_df):\n",
        "    existing_data = client.table(\"Historical Stock Dividend Data\").select(\"*\").execute().data  # Fetch existing data from Supabase\n",
        "    existing_df = pd.DataFrame(existing_data)  # Convert to DataFrame\n",
        "\n",
        "    symbols = company_df['symbol'].tolist()  # Get list of symbols\n",
        "\n",
        "    for symbol in symbols:\n",
        "        try:\n",
        "            last_date = existing_df[existing_df['symbol'] == symbol]['date'].max()  # Get the last date of existing data for the symbol\n",
        "            if pd.notnull(last_date):\n",
        "                incremental_data = fetch_incremental_stock_data(symbol, last_date)  # Fetch incremental data\n",
        "                if not incremental_data.empty:\n",
        "                    incremental_data = check_data_quality(incremental_data, symbol)  # Check data quality\n",
        "\n",
        "                    incremental_data.reset_index(inplace=True)  # Reset index to use default integer index\n",
        "                    incremental_data['symbol'] = symbol  # Add symbol column\n",
        "                    incremental_data['company_name'] = company_df[company_df['symbol'] == symbol]['company_name'].iloc[0]  # Add company name\n",
        "\n",
        "                    incremental_data['dividends'] = incremental_data.get('dividends', 0)  # Add dividends column\n",
        "                    incremental_data['dividend_yield'] = incremental_data['dividends'] / incremental_data['close'] if 'dividends' in incremental_data.columns and incremental_data['close'] != 0 else 0  # Calculate dividend yield\n",
        "\n",
        "                    # Define the headers\n",
        "                    headers = [\n",
        "                        \"record_id\", \"created_at\", \"active\", \"date\", \"company_name\", \"symbol\", \"open\", \"high\", \"low\", \"close\",\n",
        "                        \"adj_close\", \"volume\", \"dividends\", \"stock_splits\", \"dividend_yield\", \"return_on_equity\",\n",
        "                        \"earnings_per_share_eps\", \"price_to_earnings_ratio\", \"market_capitalization\", \"revenue\",\n",
        "                        \"net_income\", \"total_assets\", \"total_liabilities\", \"total_equity\", \"operating_cash_flow\",\n",
        "                        \"investing_cash_flow\", \"sma_50\", \"volatility\", \"ema\", \"model_trained\"\n",
        "                    ]\n",
        "\n",
        "                    # Placeholder values for missing columns\n",
        "                    for col in headers:\n",
        "                        if col not in incremental_data.columns:\n",
        "                            incremental_data[col] = None\n",
        "\n",
        "                    # Reindex columns to match the headers\n",
        "                    incremental_data = incremental_data.reindex(columns=headers, fill_value=None)\n",
        "\n",
        "                    # Replace out-of-range float values and NaNs\n",
        "                    incremental_data.replace([float('inf'), float('-inf')], None, inplace=True)\n",
        "                    incremental_data = incremental_data.where(pd.notnull(incremental_data), None)\n",
        "\n",
        "                    # Ensure correct format for timestamp columns\n",
        "                    incremental_data = format_timestamps(incremental_data)\n",
        "\n",
        "                    # Serialize complex fields to JSON\n",
        "                    incremental_data = serialize_complex_fields(incremental_data)\n",
        "\n",
        "                    # Convert boolean columns to integers (Supabase does not support boolean values)\n",
        "                    boolean_columns = incremental_data.select_dtypes(include=[bool]).columns.tolist()\n",
        "                    for col in boolean_columns:\n",
        "                        incremental_data[col] = incremental_data[col].astype(int)\n",
        "\n",
        "                    # Handle NaN values in critical columns\n",
        "                    critical_columns = ['date', 'adj_close', 'sma_50', 'volatility']\n",
        "                    for col in critical_columns:\n",
        "                        if col in incremental_data.columns:\n",
        "                            incremental_data[col].fillna(0, inplace=True)  # Or use another appropriate value\n",
        "                            incremental_data[col] = incremental_data[col].apply(lambda x: 0 if pd.isnull(x) else x)  # Ensure no NaNs\n",
        "\n",
        "                    # Log request data before sending to Supabase\n",
        "                    logging.info(f\"Request Data for {symbol}: {incremental_data.to_dict(orient='records')}\")\n",
        "\n",
        "                    # Convert DataFrame to dictionary and insert into Supabase\n",
        "                    stock_data_dict = incremental_data.to_dict(orient='records')\n",
        "                    for record in stock_data_dict:\n",
        "                        client.table(\"Historical Stock Dividend Data\").upsert(record).execute()\n",
        "                    logging.info(f\"Successfully updated data for {symbol}\")\n",
        "                else:\n",
        "                    logging.warning(f\"No incremental data fetched for {symbol}\")\n",
        "            else:\n",
        "                logging.warning(f\"No existing data for {symbol}, skipping incremental update\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error updating data for {symbol}: {e}\")\n",
        "            send_alert(f\"Error updating data for {symbol}: {e}\")\n",
        "\n",
        "# Usage example\n",
        "company_df = pd.DataFrame([{\"symbol\": \"AAPL\", \"company_name\": \"Apple Inc.\"}])  # Example DataFrame setup\n",
        "update_supabase_with_incremental_data(client, company_df)\n"
      ],
      "metadata": {
        "id": "C2sBWsWEKKSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Quality Checks**"
      ],
      "metadata": {
        "id": "HZXLHeR8KmXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging  # Import logging for logging messages\n",
        "import datetime  # Import datetime for date manipulation\n",
        "import pandas as pd  # Import pandas for data manipulation\n",
        "from retrying import retry  # Import retrying for retry mechanism\n",
        "from supabase import create_client  # Import Supabase client for database operations\n",
        "import yfinance as yf  # Import yfinance for fetching stock data\n",
        "import os  # Import os for environment variable handling\n",
        "import json  # Import json for JSON handling\n",
        "\n",
        "# Set up logging configuration\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[\n",
        "    logging.FileHandler(\"stock_data_update.log\"),  # Log to a file\n",
        "    logging.StreamHandler()  # Log to the console\n",
        "])\n",
        "\n",
        "# Initialize Supabase client\n",
        "supabase_url = \"https://zphgtxuwecitedpxrahe.supabase.co\"\n",
        "supabase_key = os.getenv('SUPABASE_KEY')  # Ensure this environment variable is set\n",
        "client = create_client(supabase_url, supabase_key)  # Create Supabase client\n",
        "\n",
        "# Retry configuration for fetching data\n",
        "@retry(wait_fixed=2000, stop_max_attempt_number=5)\n",
        "def fetch_incremental_stock_data(symbol, last_date):\n",
        "    logging.info(f\"Fetching incremental data for {symbol} from {last_date}\")\n",
        "    start_date = (pd.to_datetime(last_date) + pd.DateOffset(1)).strftime('%Y-%m-%d')\n",
        "    end_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
        "    stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
        "    if stock_data.empty:\n",
        "        raise ValueError(f\"No data fetched for {symbol}\")\n",
        "    return stock_data\n",
        "\n",
        "# Function to check data quality\n",
        "def check_data_quality(df, symbol):\n",
        "    if df.isnull().values.any():  # Check for null values\n",
        "        logging.warning(f\"Data for {symbol} contains null values.\")\n",
        "        # Handle null values, e.g., drop them or fill with a default value\n",
        "        df = df.dropna()\n",
        "\n",
        "    if df.duplicated().any():  # Check for duplicate rows\n",
        "        logging.warning(f\"Data for {symbol} contains duplicate rows.\")\n",
        "        # Remove duplicate rows\n",
        "        df = df.drop_duplicates()\n",
        "\n",
        "    # Additional data quality checks can be added here\n",
        "    return df\n",
        "\n",
        "# Function to ensure correct format for timestamp columns\n",
        "def format_timestamps(data):\n",
        "    if 'created_at' in data.columns and data['created_at'].dtype == 'datetime64[ns]':\n",
        "        data['created_at'] = data['created_at'].apply(lambda x: x.isoformat() if pd.notnull(x) else None)\n",
        "    return data\n",
        "\n",
        "# Function to serialize complex fields to JSON\n",
        "def serialize_complex_fields(data):\n",
        "    for column in data.columns:\n",
        "        if isinstance(data[column].iloc[0], (dict, list)):\n",
        "            data[column] = data[column].apply(lambda x: json.dumps(x) if pd.notnull(x) else None)\n",
        "    return data\n",
        "\n",
        "# Function to update Supabase with incremental data\n",
        "def update_supabase_with_incremental_data(client, company_df):\n",
        "    existing_data = client.table(\"Historical Stock Dividend Data\").select(\"*\").execute().data  # Fetch existing data from Supabase\n",
        "    existing_df = pd.DataFrame(existing_data)  # Convert to DataFrame\n",
        "\n",
        "    symbols = company_df['symbol'].tolist()  # Get list of symbols\n",
        "\n",
        "    for symbol in symbols:\n",
        "        try:\n",
        "            last_date = existing_df[existing_df['symbol'] == symbol]['date'].max()  # Get the last date of existing data for the symbol\n",
        "            if pd.notnull(last_date):\n",
        "                incremental_data = fetch_incremental_stock_data(symbol, last_date)  # Fetch incremental data\n",
        "                if not incremental_data.empty:\n",
        "                    incremental_data = check_data_quality(incremental_data, symbol)  # Check data quality\n",
        "\n",
        "                    incremental_data.reset_index(inplace=True)  # Reset index to use default integer index\n",
        "                    incremental_data['symbol'] = symbol  # Add symbol column\n",
        "                    incremental_data['company_name'] = company_df[company_df['symbol'] == symbol]['company_name'].iloc[0]  # Add company name\n",
        "\n",
        "                    incremental_data['dividends'] = incremental_data.get('dividends', 0)  # Add dividends column\n",
        "                    incremental_data['dividend_yield'] = incremental_data['dividends'] / incremental_data['close'] if 'dividends' in incremental_data.columns and incremental_data['close'] != 0 else 0  # Calculate dividend yield\n",
        "\n",
        "                    # Define the headers\n",
        "                    headers = [\n",
        "                        \"record_id\", \"created_at\", \"active\", \"date\", \"company_name\", \"symbol\", \"open\", \"high\", \"low\", \"close\",\n",
        "                        \"adj_close\", \"volume\", \"dividends\", \"stock_splits\", \"dividend_yield\", \"return_on_equity\",\n",
        "                        \"earnings_per_share_eps\", \"price_to_earnings_ratio\", \"market_capitalization\", \"revenue\",\n",
        "                        \"net_income\", \"total_assets\", \"total_liabilities\", \"total_equity\", \"operating_cash_flow\",\n",
        "                        \"investing_cash_flow\", \"sma_50\", \"volatility\", \"ema\", \"model_trained\"\n",
        "                    ]\n",
        "\n",
        "                    # Placeholder values for missing columns\n",
        "                    for col in headers:\n",
        "                        if col not in incremental_data.columns:\n",
        "                            incremental_data[col] = None\n",
        "\n",
        "                    # Reindex columns to match the headers\n",
        "                    incremental_data = incremental_data.reindex(columns=headers, fill_value=None)\n",
        "\n",
        "                    # Replace out-of-range float values and NaNs\n",
        "                    incremental_data.replace([float('inf'), float('-inf')], None, inplace=True)\n",
        "                    incremental_data = incremental_data.where(pd.notnull(incremental_data), None)\n",
        "\n",
        "                    # Ensure correct format for timestamp columns\n",
        "                    incremental_data = format_timestamps(incremental_data)\n",
        "\n",
        "                    # Serialize complex fields to JSON\n",
        "                    incremental_data = serialize_complex_fields(incremental_data)\n",
        "\n",
        "                    # Convert boolean columns to integers (Supabase does not support boolean values)\n",
        "                    boolean_columns = incremental_data.select_dtypes(include=[bool]).columns.tolist()\n",
        "                    for col in boolean_columns:\n",
        "                        incremental_data[col] = incremental_data[col].astype(int)\n",
        "\n",
        "                    # Handle NaN values in critical columns\n",
        "                    critical_columns = ['date', 'adj_close', 'sma_50', 'volatility']\n",
        "                    for col in critical_columns:\n",
        "                        if col in incremental_data.columns:\n",
        "                            incremental_data[col].fillna(0, inplace=True)  # Or use another appropriate value\n",
        "                            incremental_data[col] = incremental_data[col].apply(lambda x: 0 if pd.isnull(x) else x)  # Ensure no NaNs\n",
        "\n",
        "                    # Log request data before sending to Supabase\n",
        "                    logging.info(f\"Request Data for {symbol}: {incremental_data.to_dict(orient='records')}\")\n",
        "\n",
        "                    # Convert DataFrame to dictionary and insert into Supabase\n",
        "                    stock_data_dict = incremental_data.to_dict(orient='records')\n",
        "                    for record in stock_data_dict:\n",
        "                        client.table(\"Historical Stock Dividend Data\").upsert(record).execute()\n",
        "                    logging.info(f\"Successfully updated data for {symbol}\")\n",
        "                else:\n",
        "                    logging.warning(f\"No incremental data fetched for {symbol}\")\n",
        "            else:\n",
        "                logging.warning(f\"No existing data for {symbol}, skipping incremental update\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error updating data for {symbol}: {e}\")\n",
        "\n",
        "# Usage example\n",
        "company_df = pd.DataFrame([{\"symbol\": \"AAPL\", \"company_name\": \"Apple Inc.\"}])  # Example DataFrame setup\n",
        "update_supabase_with_incremental_data(client, company_df)\n"
      ],
      "metadata": {
        "id": "Mc7QKO3pKpbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Model Confirmation**"
      ],
      "metadata": {
        "id": "cqIze3bHNqyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging  # Import logging for logging messages\n",
        "import pandas as pd  # Import pandas for data manipulation\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score  # Import sklearn for model training\n",
        "from sklearn.ensemble import RandomForestRegressor  # Import RandomForestRegressor for regression tasks\n",
        "from sklearn.metrics import mean_squared_error, r2_score  # Import metrics for evaluating models\n",
        "from datetime import datetime  # Import datetime for time handling\n",
        "import smtplib  # Import smtplib for sending emails\n",
        "from email.mime.text import MIMEText  # Import MIMEText for email text handling\n",
        "from email.mime.multipart import MIMEMultipart  # Import MIMEMultipart for email multipart handling\n",
        "from supabase import create_client  # Import Supabase client for database operations\n",
        "import os  # Import os for environment variable handling\n",
        "import json  # Import json for JSON handling\n",
        "import pickle  # Import pickle for saving models\n",
        "\n",
        "# Set up logging configuration\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[\n",
        "    logging.FileHandler(\"model_training.log\"),  # Log to a file\n",
        "    logging.StreamHandler()  # Log to the console\n",
        "])\n",
        "\n",
        "# Ensure environment variables are set\n",
        "SUPABASE_KEY = os.getenv('SUPABASE_KEY')\n",
        "SUPABASE_URL = os.getenv('SUPABASE_URL')\n",
        "SENDER_EMAIL = os.getenv('SENDER_EMAIL')\n",
        "RECEIVER_EMAIL = os.getenv('RECEIVER_EMAIL')\n",
        "EMAIL_PASSWORD = os.getenv('EMAIL_PASSWORD')\n",
        "\n",
        "if not SUPABASE_KEY or not SUPABASE_URL or not SENDER_EMAIL or not RECEIVER_EMAIL or not EMAIL_PASSWORD:\n",
        "    raise EnvironmentError(\"Required environment variables (SUPABASE_KEY, SUPABASE_URL, SENDER_EMAIL, RECEIVER_EMAIL, EMAIL_PASSWORD) are not set.\")\n",
        "\n",
        "# Initialize Supabase client\n",
        "client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "# Function to add features to the data\n",
        "def add_features(data):\n",
        "    data['sma_50'] = data['close'].rolling(window=50).mean()\n",
        "    data['volatility'] = data['close'].rolling(window=50).std()\n",
        "    # Add more features as needed\n",
        "    return data\n",
        "\n",
        "# Function to tune hyperparameters\n",
        "def tune_hyperparameters(X, y):\n",
        "    model = RandomForestRegressor()\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
        "    grid_search.fit(X, y)\n",
        "    return grid_search.best_estimator_\n",
        "\n",
        "# Function to evaluate the model using cross-validation\n",
        "def evaluate_model(model, X, y):\n",
        "    scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
        "    logging.info(f'Cross-validated MSE: {-scores.mean()}')\n",
        "    return -scores.mean()\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(data, target_column='close'):\n",
        "    X = data.drop(columns=[target_column])\n",
        "    y = data[target_column]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    model = tune_hyperparameters(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Evaluate the model using cross-validation\n",
        "    cv_mse = evaluate_model(model, X_train, y_train)\n",
        "\n",
        "    return model, mse, r2, cv_mse\n",
        "\n",
        "# Function to update the training confirmation database\n",
        "def update_training_confirmation_db(client, company_name, symbol, mse, r2, cv_mse, version):\n",
        "    table = client.table(\"Training Confirmation\")\n",
        "\n",
        "    # Prepare the data to be inserted\n",
        "    training_start_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    model_version = f\"v{version}\"\n",
        "    remarks = \"Training completed successfully\" if mse is not None and r2 is not None else \"Training failed\"\n",
        "\n",
        "    row = {\n",
        "        \"company_name\": company_name,\n",
        "        \"symbol\": symbol,\n",
        "        \"model_training_successful\": True,\n",
        "        \"mse\": mse,\n",
        "        \"r2\": r2,\n",
        "        \"cross_validated_mse\": cv_mse,\n",
        "        \"training_start_time\": training_start_time,\n",
        "        \"training_end_time\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        \"model_version\": model_version,\n",
        "        \"remarks\": remarks\n",
        "    }\n",
        "\n",
        "    # Insert the data into the Supabase table\n",
        "    table.insert(row).execute()\n",
        "    logging.info(f\"Training confirmation for {symbol} updated successfully with version {model_version}.\")\n",
        "\n",
        "# Function to send email report\n",
        "def send_email_report(company_name, symbol, mse, r2, cv_mse, model_version):\n",
        "    # Email details\n",
        "    subject = f\"Model Training Report for {company_name} ({symbol})\"\n",
        "    body = f\"\"\"\n",
        "    Company: {company_name}\n",
        "    Symbol: {symbol}\n",
        "    Model Version: {model_version}\n",
        "    Mean Squared Error: {mse}\n",
        "    R-squared: {r2}\n",
        "    Cross-Validated MSE: {cv_mse}\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the email\n",
        "    msg = MIMEMultipart()\n",
        "    msg['From'] = SENDER_EMAIL\n",
        "    msg['To'] = RECEIVER_EMAIL\n",
        "    msg['Subject'] = subject\n",
        "    msg.attach(MIMEText(body, 'plain'))\n",
        "\n",
        "    # Send the email\n",
        "    try:\n",
        "        server = smtplib.SMTP('smtp.gmail.com', 587)\n",
        "        server.starttls()\n",
        "        server.login(SENDER_EMAIL, EMAIL_PASSWORD)\n",
        "        text = msg.as_string()\n",
        "        server.sendmail(SENDER_EMAIL, RECEIVER_EMAIL, text)\n",
        "        server.quit()\n",
        "        logging.info(f\"Email report for {symbol} sent successfully.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to send email report for {symbol}: {e}\")\n",
        "\n",
        "# Function to get the latest model version\n",
        "def get_latest_model_version(symbol):\n",
        "    # Fetch the latest model version from Supabase\n",
        "    response = client.table(\"Training Confirmation\").select(\"model_version\").eq(\"symbol\", symbol).execute()\n",
        "    if response.data:\n",
        "        versions = [int(item['model_version'][1:]) for item in response.data if item['model_version'].startswith('v')]\n",
        "        return max(versions) if versions else 0\n",
        "    return 0\n",
        "\n",
        "# Function to save the model to drive\n",
        "def save_model_to_drive(model, symbol, version):\n",
        "    model_filename = f\"{symbol}_model_v{version}.pkl\"\n",
        "    with open(model_filename, 'wb') as model_file:\n",
        "        pickle.dump(model, model_file)\n",
        "    logging.info(f\"Model saved to drive with filename: {model_filename}\")\n",
        "\n",
        "# Function to process and train models\n",
        "def process_and_train_models(client, company_df, fetched_data):\n",
        "    for symbol, data in fetched_data.items():\n",
        "        try:\n",
        "            if not data.empty:\n",
        "                data.reset_index(inplace=True)\n",
        "                data = add_features(data)\n",
        "                data = check_data_quality(data, symbol)  # Ensure data quality checks are applied\n",
        "\n",
        "                company_name = company_df[company_df['symbol'] == symbol]['company_name'].iloc[0]\n",
        "\n",
        "                # Train model\n",
        "                model, mse, r2, cv_mse = train_model(data)\n",
        "\n",
        "                # Get the latest version of the model\n",
        "                latest_version = get_latest_model_version(symbol)\n",
        "                new_version = latest_version + 1\n",
        "\n",
        "                # Save the new model version to drive\n",
        "                save_model_to_drive(model, symbol, new_version)\n",
        "\n",
        "                # Update Supabase with training confirmation including version control\n",
        "                update_training_confirmation_db(client, company_name, symbol, mse, r2, cv_mse, new_version)\n",
        "\n",
        "                # Send the email report\n",
        "                send_email_report(company_name, symbol, mse, r2, cv_mse, new_version)\n",
        "\n",
        "                # Update Historical Stock Dividend Data table to mark the data as trained\n",
        "                client.table(\"Historical Stock Dividend Data\").update({\"model_trained\": True}).eq(\"symbol\", symbol).execute()\n",
        "\n",
        "                logging.info(f\"Model training and version control for {symbol} completed.\")\n",
        "            else:\n",
        "                logging.warning(f\"No data available for training model for {symbol}.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error processing and training model for {symbol}: {e}\")\n",
        "\n",
        "# Usage example\n",
        "# Fetch company_df from your data source, e.g., Supabase\n",
        "# fetched_data = dict(zip(symbols, results))\n",
        "# process_and_train_models(client, company_df, fetched_data)\n"
      ],
      "metadata": {
        "id": "mtPX-N8cNyS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Version Control for Models**"
      ],
      "metadata": {
        "id": "GQaUlrdsOZ5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os  # Import os for environment variable handling\n",
        "import pickle  # Import pickle for saving models\n",
        "import logging  # Import logging for logging messages\n",
        "from datetime import datetime  # Import datetime for time handling\n",
        "from supabase import create_client  # Import Supabase client for database operations\n",
        "\n",
        "# Set up logging configuration\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[\n",
        "    logging.FileHandler(\"model_training.log\"),  # Log to a file\n",
        "    logging.StreamHandler()  # Log to the console\n",
        "])\n",
        "\n",
        "# Ensure the environment variables are set\n",
        "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
        "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
        "SENDER_EMAIL = os.getenv(\"SENDER_EMAIL\")\n",
        "RECEIVER_EMAIL = os.getenv(\"RECEIVER_EMAIL\")\n",
        "EMAIL_PASSWORD = os.getenv(\"EMAIL_PASSWORD\")\n",
        "\n",
        "if not SUPABASE_URL or not SUPABASE_KEY or not SENDER_EMAIL or not RECEIVER_EMAIL or not EMAIL_PASSWORD:\n",
        "    raise EnvironmentError(\"Required environment variables (SUPABASE_URL, SUPABASE_KEY, SENDER_EMAIL, RECEIVER_EMAIL, EMAIL_PASSWORD) are not set.\")\n",
        "\n",
        "# Initialize Supabase client\n",
        "client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "def save_model_to_drive(model, model_name, version):\n",
        "    \"\"\"Save the model to a local directory with version control.\"\"\"\n",
        "    # Create a directory for storing models if it doesn't exist\n",
        "    model_dir = f\"models/{model_name}\"  # Local path for simplicity; adjust if using cloud storage\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "\n",
        "    # Save the model as a pickle file\n",
        "    model_path = os.path.join(model_dir, f\"{model_name}_v{version}.pkl\")\n",
        "    with open(model_path, 'wb') as model_file:\n",
        "        pickle.dump(model, model_file)\n",
        "\n",
        "    logging.info(f\"Model saved to {model_path}\")\n",
        "\n",
        "def get_latest_model_version(model_name):\n",
        "    \"\"\"Get the latest version of the model from the local directory.\"\"\"\n",
        "    model_dir = f\"models/{model_name}\"  # Local path for simplicity; adjust if using cloud storage\n",
        "    if not os.path.exists(model_dir):\n",
        "        return 0\n",
        "\n",
        "    # Get the latest version number\n",
        "    versions = [int(f.split('_v')[1].split('.pkl')[0]) for f in os.listdir(model_dir) if f.endswith('.pkl')]\n",
        "    return max(versions) if versions else 0\n",
        "\n",
        "def update_training_confirmation_db_with_version(client, company_name, symbol, mse, r2, cv_mse, version):\n",
        "    \"\"\"Update the training confirmation database with version control.\"\"\"\n",
        "    table = client.table(\"Training Confirmation\")\n",
        "\n",
        "    # Prepare the data to be inserted\n",
        "    training_start_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    model_version = f\"v{version}\"\n",
        "    remarks = \"Training completed successfully\" if mse is not None and r2 is not None else \"Training failed\"\n",
        "\n",
        "    row = {\n",
        "        \"company_name\": company_name,\n",
        "        \"symbol\": symbol,\n",
        "        \"model_training_successful\": True,\n",
        "        \"mse\": mse,\n",
        "        \"r2\": r2,\n",
        "        \"cross_validated_mse\": cv_mse,\n",
        "        \"training_start_time\": training_start_time,\n",
        "        \"training_end_time\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        \"model_version\": model_version,\n",
        "        \"remarks\": remarks\n",
        "    }\n",
        "\n",
        "    # Insert the data into the Supabase table\n",
        "    table.insert(row).execute()\n",
        "    logging.info(f\"Training confirmation for {symbol} updated successfully with version {model_version}.\")\n",
        "\n",
        "def process_and_train_models_with_version_control(client, company_df, fetched_data):\n",
        "    \"\"\"Process and train models with version control.\"\"\"\n",
        "    for symbol, data in fetched_data.items():\n",
        "        try:\n",
        "            if not data.empty:\n",
        "                data.reset_index(inplace=True)\n",
        "                data = add_features(data)\n",
        "                data = check_data_quality(data, symbol)  # Ensure data quality checks are applied\n",
        "\n",
        "                company_name = company_df[company_df['symbol'] == symbol]['company_name'].iloc[0]\n",
        "\n",
        "                # Train model\n",
        "                model, mse, r2, cv_mse = train_model(data)\n",
        "\n",
        "                # Get the latest version of the model\n",
        "                latest_version = get_latest_model_version(symbol)\n",
        "                new_version = latest_version + 1\n",
        "\n",
        "                # Save the new model version to local directory\n",
        "                save_model_to_drive(model, symbol, new_version)\n",
        "\n",
        "                # Update Supabase with training confirmation including version control\n",
        "                update_training_confirmation_db_with_version(client, company_name, symbol, mse, r2, cv_mse, new_version)\n",
        "\n",
        "                # Mark the row of data as trained\n",
        "                client.table(\"Historical Stock Dividend Data\").update({\"model_trained\": True}).eq(\"symbol\", symbol).execute()\n",
        "\n",
        "                logging.info(f\"Model training and version control for {symbol} completed.\")\n",
        "            else:\n",
        "                logging.warning(f\"No data available for training model for {symbol}.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error processing and training model for {symbol}: {e}\")\n",
        "\n",
        "# Usage example\n",
        "# Fetch company_df from your data source, e.g., Supabase\n",
        "# fetched_data = dict(zip(symbols, results))\n",
        "# process_and_train_models_with_version_control(client, company_df, fetched_data)\n"
      ],
      "metadata": {
        "id": "vqCh9eXXOcTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main Execution Loop**"
      ],
      "metadata": {
        "id": "yNc8Pijob6aI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import logging\n",
        "import os\n",
        "from supabase import create_client, Client\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Initialize Supabase connection\n",
        "url = os.getenv(\"SUPABASE_URL\")\n",
        "key = os.getenv(\"SUPABASE_KEY\")\n",
        "supabase: Client = create_client(url, key)\n",
        "\n",
        "# Constants\n",
        "PROGRESS_FILE = 'progress.csv'\n",
        "BATCH_SIZE = 10\n",
        "START_DATE = \"2018-01-01\"\n",
        "END_DATE = None  # Remove end date\n",
        "\n",
        "# Load progress\n",
        "def load_progress(progress_file):\n",
        "    if os.path.exists(progress_file):\n",
        "        return pd.read_csv(progress_file)['symbol'].tolist()\n",
        "    return []\n",
        "\n",
        "# Save progress\n",
        "def save_progress(progress_file, processed_symbols):\n",
        "    pd.DataFrame({'symbol': processed_symbols}).to_csv(progress_file, index=False)\n",
        "\n",
        "# Fetch company data from Supabase\n",
        "response = supabase.table(\"Stock Company Information\").select(\"*\").execute()\n",
        "company_df = pd.DataFrame(response.data)\n",
        "symbols = company_df['symbol'].tolist()\n",
        "\n",
        "# Example data processing and storing in Supabase\n",
        "processed_symbols = load_progress(PROGRESS_FILE)\n",
        "symbols_to_process = company_df[~company_df['symbol'].isin(processed_symbols)]\n",
        "\n",
        "for i in range(0, len(symbols_to_process), BATCH_SIZE):\n",
        "    batch = symbols_to_process.iloc[i:i + BATCH_SIZE]\n",
        "    symbols = batch['symbol'].tolist()\n",
        "    stock_data_batch = fetch_stock_data(symbols, START_DATE, END_DATE)\n",
        "\n",
        "    for _, company in batch.iterrows():\n",
        "        symbol = company['symbol']\n",
        "        if symbol in stock_data_batch:\n",
        "            stock_data = stock_data_batch[symbol]\n",
        "            # Validate data and store in Supabase\n",
        "            validate_and_store_data(supabase, {symbol: stock_data}, company_df)\n",
        "            processed_symbols.append(symbol)\n",
        "            save_progress(PROGRESS_FILE, processed_symbols)\n",
        "        else:\n",
        "            logging.warning(f\"No data fetched for {symbol}\")\n"
      ],
      "metadata": {
        "id": "vT6D5RU2b-zT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}